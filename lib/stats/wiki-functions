
get_html()
{
    if [ "$DOWNLOAD" = "true" ] || [ ! -f  ${page}.html ]
    then
        rm -f ${page}.html
        curl -s "http://rameau.sandklef.com/mediawiki/index.php/${page}" -o ${page}.html
    fi
}

get_wiki_page()
{
    PAGES="$1"
    for page in $PAGES
    do
        echo "        \"name\": \"$page\","
        if [ "$DOWNLOAD" = "true" ] || [ ! -f  ${page}.pdf ]
        then
            HTML_DOC_ARGS="--charset iso-8859-1  --no-title --format pdf14 --numbered --firstpage toc"
            HTML_DOC_ARGS="$HTML_DOC_ARGS --fontsize 8  --fontspacing 1  --linkstyle plain --linkcolor 217A28"
            HTML_DOC_ARGS="$HTML_DOC_ARGS --toclevels 3  --headfootsize 8 --quiet --jpeg --color"
            HTML_DOC_ARGS="$HTML_DOC_ARGS --bodyfont Arial --left 1cm --top 1cm --bottom 1cm --right 1cm"
            HTML_DOC_ARGS="$HTML_DOC_ARGS   --header ... --footer .1. -t pdf"
            rm -f ${page}.pdf
#	    log_to_file	    "curl -s \"http://rameau.sandklef.com/mediawiki/index.php?title=${page}&action=pdfbook&format=single\" -o ${page}.pdf "
            #curl -s "http://rameau.sandklef.com/mediawiki/index.php?title=${page}&action=pdfbook&format=single" -o ${page}.pdf 
            htmldoc "http://wiki.juneday.se/mediawiki/index.php?title=${page}" --outfile ${page}.pdf 2>/dev/null >/dev/null
            log_to_file "htmldoc returned $?"
        fi
        PAGES=$(pdfinfo ${page}.pdf 2>/dev/null | grep Pages | awk ' { print $2}')
        echo "        \"pages\": \"$PAGES\","
        PDFS="$PDFS ${page}.pdf"
    done
    TOTAL_PAGE_COUNT=$(( TOTAL_PAGE_COUNT + PAGES ))

}

check_pres_pdfs_vids()
{
    PRES_PAGE_COUNT=0
    PRES_VIDEO_COUNT=0
    PRES_CHANNEL_COUNT=0
    UNIQ_PDFS=$(echo "$PRES_PDFS"   | uniq | tr '[\n]' '[ ]')
    UNIQ_VID_COUNT=$(echo "$PRES_VIDEOS" | uniq | wc -l)
    UNIQ_CHANNEL_COUNT=$(echo "$PRES_CHANNELS" | uniq | wc -l )
#    echo "VID: $PRES_VIDEOS"
 #   echo "CHA: $PRES_CHANNELS"
    PDF_COUNTER=0
    #["
    for pdf_long in $UNIQ_PDFS
    do
        PDF_COUNTER=$(( PDF_COUNTER + 1 ))
        pdf=$(basename "$pdf_long")
#        echo -n "   { "
 #       echo -n "     \"name\":  \"$pdf\""
        PRES_PAGES=$(pdfinfo $pdf 2>/dev/null | grep Pages | awk ' { print $2}')
  #      echo -n "     \"pages\":  \"$PRES_PAGE_COUNT\""
   #     echo -n "   }, "
        PRES_PAGE_COUNT=$(( PRES_PAGE_COUNT + PRES_PAGES ))
#        echo "$PRES_PAGES"
    done
    echo "  \"book-summary\": {"
    echo "    \"books\": \"${BOOK_COUNT}\", "
    echo "    \"pages\": \"${TOTAL_PAGE_COUNT}\","
    echo "    \"uniq-presentations\": \"$PDF_COUNTER\", "
    echo "    \"uniq-presentations-pages\": \"$PRES_PAGE_COUNT\", "
    echo "    \"uniq-videos\": \"$UNIQ_VID_COUNT\", "
    echo "    \"uniq-channels\": \"$UNIQ_CHANNEL_COUNT\" "
    echo "  },"
    TOTAL_PRES_PAGE_COUNT=$(( TOTAL_PRES_PAGE_COUNT + PRES_PAGE_COUNT ))
#    echo "    ]"
#    echo "presentaion|page-count: $PRES_PAGE_COUNT"
}

check_pdfs()
{
    book=$1
    
    PAGE_COUNT=0
    echo "  \"wiki-pages\": ["
    for pdf in $PDFS
    do
        echo "      {"
        echo "        \"name\": \"$pdf\", "
        PAGES=$(pdfinfo $pdf 2>/dev/null | grep Pages | awk ' { print $2}')
        echo "        \"pages\": \"$PAGES\", "
        PAGE_COUNT=$(( PAGE_COUNT + PAGES ))
        echo "      },"
    done
    echo "    ]"
    TOTAL_PAGE_COUNT=$(( TOTAL_PAGE_COUNT + PAGE_COUNT ))
    echo "   \"pages-total\": \"$PAGE_COUNT\""
}

get_pres_and_vids()
{
    get_html "$page"
    echo "        \"presentations\":[" 
    CHAPTER_PDF_PAGES=0
    LOCAL_PRES_VIDEO_COUNT=0
    HTML_PAGE="$1".html
    LOCAL_PRES_PDFS=$(grep "href=" "$HTML_PAGE" | grep pdf | sed 's, ,\n,g' | grep pdf | grep mediawiki | sed -e 's,href=,,g' -e 's,",,g')
    LOCAL_PRES_VIDEOS=$(grep "vimeo" "$HTML_PAGE" | sed 's,[ >],\n,g' | grep href | sed -e 's,href=,,g' -e 's,",,g'  | grep -v mediawiki | grep -v github | grep -v channel)
    LOCAL_PRES_CHANNELS=$(grep "vimeo" "$HTML_PAGE" | sed 's,[ >],\n,g' | grep href | sed -e 's,href=,,g' -e 's,",,g'  | grep -v mediawiki | grep -v github | grep  channel)
    CNT=0
    for page in $LOCAL_PRES_PDFS
    do
        if [ $CNT -ne 0 ]
        then
            echo ","
        fi
        CNT=$(( CNT + 1 ))
        short_page=$(basename ${page})
        if [ "$DOWNLOAD" = "true" ] || [ ! -f  ${short_page} ]
        then
            rm -f ${short_page}
            curl -s "http://rameau.sandklef.com/$page" -o ${short_page}
        fi
        echo "            { "
        echo "              \"name\": \"$short_page\","
        PAGES=$(pdfinfo $short_page 2>/dev/null | grep Pages | awk ' { print $2}')
        CHAPTER_PDF_PAGES=$(( CHAPTER_PDF_PAGES + PAGES ))
        echo "              \"pages\": \"$PAGES\""
        echo -n "            } "
    done
    echo
    echo "          ]," 
    echo "        \"presentation-pages\": \"$CHAPTER_PDF_PAGES\"," 
    echo "        \"videos\":[" 
    LOCAL_PRES_VIDEO_COUNT=0
    for video in $LOCAL_PRES_VIDEOS
    do
        if [ $LOCAL_PRES_VIDEO_COUNT -ne 0 ]
        then
            echo ","
        fi
        LOCAL_PRES_VIDEO_COUNT=$(( LOCAL_PRES_VIDEO_COUNT + 1 ))
#        echo "              { "
#        echo "                 \"url\": \"$video\""
        echo -n "                 \"$video\""
#        echo "              }, "
    done
    PRES_VIDEO_COUNT=$(( PRES_VIDEO_COUNT + LOCAL_PRES_VIDEO_COUNT ))
    echo  "          ]," 
    echo "        \"video-count\": \"$LOCAL_PRES_VIDEO_COUNT\"," 
    echo "        \"channels\":[" 
    LOCAL_PRES_CHANNEL_COUNT=0
    for channel in $LOCAL_PRES_CHANNELS
    do
        if [ $LOCAL_PRES_CHANNEL_COUNT -ne 0 ]
        then
            echo ","
        fi
        LOCAL_PRES_CHANNEL_COUNT=$(( LOCAL_PRES_CHANNEL_COUNT + 1 ))
        echo -n "                 \"$channel\""
    done
    echo  "          ]," 
    echo "        \"channel-count\": \"$LOCAL_PRES_CHANNEL_COUNT\"" 
    PRES_CHANNEL_COUNT=$(( PRES_CHANNEL_COUNT + LOCAL_PRES_CHANNEL_COUNT ))

    PRES_PDFS="$PRES_PDFS $LOCAL_PRES_PDFS"
    PRES_VIDEOS="$PRES_VIDEOS $LOCAL_PRES_VIDEOS"
    PRES_CHANNELS="$PRES_CHANNELS $LOCAL_PRES_CHANNELS"
}


check_book()
{
    book=$1
    
    PAGES=${book}_PAGES
    TITLE_VAR=${book}_TITLE
    TITLE=${!TITLE_VAR}
    TITLE_NO_BLANKS=$(echo $TITLE | sed 's, ,_,g')

    log_to_file "PAGES: $PAGES / ${!PAGES}"
    
    debug " * $TITLE"
    echo "  {"
    echo "    \"title\": \"$TITLE\","
    PDFS=""
    echo "    \"chapters\": ["
    local COUNT=0
    for page in ${!PAGES}
    do
        if [ $COUNT -ne 0 ]
        then
            echo ","
        fi
        COUNT=$(( CNT + 1 ))
        echo "        { "

        # download pdf version of wiki page
        get_wiki_page   "$page"

        # download presentation pdfs linked from wiki page
        get_pres_and_vids "$page"
        echo "        } "
    done
    echo "  ]"
    echo " }"
    
#    echo "   \"presentations-pages-total\": \"$PRES_PAGE_COUNT\""

#    check_pdfs $book

    PAGE_COUNTS["${book}"]="${PAGE_COUNT}"
    TOTAL_PAGE_COUNT=$(( TOTAL_PAGE_COUNT + PAGE_COUNT ))
    rm -f  "${TITLE_NO_BLANKS}.pdf"
    pdfmerge $PDFS "${TITLE_NO_BLANKS}.pdf" 2>/dev/null >/dev/null
}




get_wiki_stats_sub()
{
    export EXP=$2
    NR=$(grep "$EXP" $TEMP_DIR/wiki.txt | sed "s/$EXP//g" | awk '{ print $1 }' | sed 's/,//g')
    echo -n "      \"$1\": \"$NR\""
}
get_wiki_stats()
{
    echo "  \"wiki-stats\": { "
    w3m -dump http://rameau.sandklef.com/mediawiki/index.php/Special:Statistics > $TEMP_DIR/wiki.txt
    get_wiki_stats_sub "content-pages" "Content pages"
    echo ","
    get_wiki_stats_sub "pages" "Pages"
    echo ","
    get_wiki_stats_sub "uploaded-files" "Uploaded files"
    echo ","
    get_wiki_stats_sub "edits" "Page edits since Juneday education was set up"
    echo "  }"
}
